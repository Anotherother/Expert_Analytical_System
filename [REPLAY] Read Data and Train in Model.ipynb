{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import src.replay_detection as rd_features\n",
    "import src.make_plot as plt_samples\n",
    "import src.evaluation_plot as ev_plot\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_original = pd.read_csv('./computedFeatures/data_train_original.csv')\n",
    "data_dev_original   = pd.read_csv('./computedFeatures/data_dev_original.csv') \n",
    "data_eva_original   = pd.read_csv('./computedFeatures/data_eva_original.csv') \n",
    "\n",
    "data_train_spoof = pd.read_csv('./computedFeatures/data_train_spoof.csv')\n",
    "data_dev_spoof   = pd.read_csv('./computedFeatures/data_dev_spoof.csv') \n",
    "data_eva_spoof   = pd.read_csv('./computedFeatures/data_eva_spoof.csv') \n",
    "\n",
    "# load labels for test set. Another u can upload from /protocol/* ... read as pandas DataFrame\n",
    "\n",
    "y_test = np.loadtxt('./protocol/y_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scripts for learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GMM-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GMM\n",
    "\n",
    "def GMMLearn(X_train_original, X_train_spoof,X_test, y_test, n_g):\n",
    "    \"\"\"\n",
    "    n_g - number of Gaussian\n",
    "    \"\"\"\n",
    "    ### Здесь еще проверь вызов функций. С 0.19 версии sklearn обновился вызов GMM. \n",
    "    ### Теперь величаем как GaussianMixture.Возможно и аргументы поменялись \n",
    "    \n",
    "    g1m =  GMM(n_components = n_g, covariance_type='diag',init_params='wmc', n_iter=30)\n",
    "    g1m.fit(X_train_original)\n",
    "\n",
    "    g2m =  GMM(n_components = n_g, covariance_type='diag',init_params='wmc', n_iter=30)\n",
    "    g2m.fit(X_train_spoof)\n",
    "\n",
    "    scores = g1m.score(X_test)  - g2m.score(X_test)\n",
    "\n",
    "    plotROC(y_test, scores, \"scores\")\n",
    "\n",
    "    eer, thr = ev_plot.EER_THR(y_test, scores)\n",
    "    print (\"n_g = %d, EER = %f, thr = %lf\" % (n_g, eer, thr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = GMMLearn(fetureVecTrainOriginal, fetureVecTrainSpoof, featureVecTest, y_test, n_g = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GMM-UBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeUBM(ubm_model, data):\n",
    "    \n",
    "    ###########################################\n",
    "    # ubm_model - gmm-represent distribution of our model\n",
    "    # data - samples, which will correct ubm-model\n",
    "    ###############################################\n",
    "    \n",
    "    xdim = data.shape[1]\n",
    "    M = ubm_model.n_components\n",
    "    \n",
    "    ###############################################################   \n",
    "    #    ubm_means: means of the ubm <number array>               #\n",
    "    #    ubm_covars: covariances of the ubm <number array>        #\n",
    "    #    ubm_weights: weights of the ubm <number array>           #\n",
    "    #    new_means: means adapted from the ubm <number array>     #\n",
    "    #    new_weights: weights adapted from the ubm <number array> #\n",
    "    ###############################################################    \n",
    "        \n",
    "    # Copy parameters GMM-model\n",
    "    ubm_weights = ubm_model.weights_\n",
    "    ubm_means = ubm_model.means_\n",
    "    ubm_covars = ubm_model.covars_\n",
    "           \n",
    "    ###################################################################\n",
    "    # for X = {x_1, ..., x_T}                                         # \n",
    "    # P(i|x_t) = w_i * p_i(x_t) / sum_j=1_M(w_j * P_j(x_t))           #\n",
    "    ###################################################################\n",
    "    \n",
    "    posterior_prob = ubm_model.predict_proba(data)\n",
    "    pr_i_xt = (ubm_weights * posterior_prob)/ np.asmatrix(np.sum(ubm_weights \\\n",
    "                                                * posterior_prob, axis = 1)).T\n",
    "\n",
    "    n_i = np.asarray(np.sum(pr_i_xt, axis = 0)).flatten() # [M, ]\n",
    "    \n",
    "    # Then we can compute E(x) and E(x2) and calculate new parameters of\n",
    "    # our model\n",
    "    \n",
    "    E_x = np.asarray([(np.asarray(pr_i_xt[:, i]) * data).sum(axis = 0) / n_i[i] for i in range(M)]) # [M x xdim]  \n",
    "    E_x2 = np.asarray([(np.asarray(pr_i_xt[:, i]) * (data**2)).sum(axis = 0) / n_i[i] for i in range(M)])# [M x xdim]\n",
    "\n",
    "    ################################################################ \n",
    "    #    T: scaling factor, number of samples                      #\n",
    "    #    relevance_factor: factor for scaling the adapted means    #\n",
    "    #    scaleparam - scale parameter for weights matrix estimation#\n",
    "    ################################################################  \n",
    "    \n",
    "    T = data.shape[0]   \n",
    "    relevance_factor = 16\n",
    "    scaleparam = 1\n",
    "    \n",
    "    ################################################################      \n",
    "    # compute alpha_i: data-depentend apaptation coefficient       #\n",
    "    # alpha_w = alpha_m = alpha_v                                  #\n",
    "    # alpha_i = n_i/ (n_i + relevance factor)                      #\n",
    "    ################################################################\n",
    "    \n",
    "    alpha_i = n_i / (n_i + relevance_factor)\n",
    "\n",
    "    ###############################\n",
    "    # Parqameter`s adaptation\n",
    "    ##############################\n",
    "    new_weights = (alpha_i * n_i / T + (1.0 - alpha_i)* ubm_weights) * scaleparam       \n",
    "    \n",
    "    alpha_i = np.asarray(np.asmatrix(alpha_i).T)    \n",
    "    new_means = (alpha_i * E_x + (1. - alpha_i) * ubm_means)      \n",
    "    new_covars = alpha_i * E_x2 + (1. - alpha_i) * (ubm_covars + (ubm_means **2)) - (new_means ** 2)\n",
    "\n",
    "    #############################################\n",
    "    #if we want compute `full` covariance matrix - comment code here\n",
    "    #new_covars = np.zeros([M, xdim, xdim])\n",
    "    #for j in range(M):\n",
    "    #    new_covars[j] = alpha_i[j]*E_x2[j] +(1. - alpha_i[j]).flatten()*(ubm_covars[j] + (new_means[j]**2))- (ubm_means[j]**2)\n",
    "    #    new_covars[i] = np.where(new_covars[i] < MIN_VARIANCE, MIN_VARIANCE, new_covars[i])\n",
    "    ####################################################################\n",
    "    #   `covars_` : array\n",
    "    #    Covariance parameters for each mixture component.  The shape\n",
    "    #    depends on `covariance_type`::\n",
    "    #        (n_components, n_features)             if 'spherical',\n",
    "    #        (n_features, n_features)               if 'tied',\n",
    "    #        (n_components, n_features)             if 'diag',\n",
    "    #        (n_components, n_features, n_features) if 'full'\n",
    "    #####################################################################\n",
    "\n",
    "    \n",
    "    ubm_model.means_ = new_means\n",
    "    ubm_model.weights_ = new_weights\n",
    "    ubm_model.covars_ = new_covars\n",
    "\n",
    "    return ubm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### USAGE:\n",
    "# create one dataset, included all data - spoofing and original \n",
    "# for example: X_train = pd.concat(data_train_original, data_train_spoof)\n",
    "\n",
    "model = GMM(n_components = 200, covariance_type='diag',init_params='wmc', n_iter=20)\n",
    "model.fit(X_train)\n",
    "\n",
    "# after adapt data on development set - shifting to  original and spoofing data\n",
    "\n",
    "adapt_original = computeUBM(model, mfcc_original_develop)\n",
    "adapt_spoof = computeUBM(model, mfcc_spoof_develop)\n",
    "\n",
    "\n",
    "prediction  = np.array(adapt_original.score(X_test)  < adapt_spoof.score(X_test)).astype('int')\n",
    "\n",
    "accuracy = np.mean(prediction == y_test) * 100\n",
    "print ('accuracy - prediction on evaluation set', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_dev_scaled = scaler.transform(X_dev)\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "prediciton = clf.predict(X_dev_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
